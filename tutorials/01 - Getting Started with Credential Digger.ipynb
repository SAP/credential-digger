{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Getting started with Credential Digger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to Credential Digger's introductory tutorial, to detect hardcoded credentials in GitHub repositories. In this notebook, we are going to see the main functionalities of Credential Digger, its inner workings and how to personalize it.\n",
    "\n",
    "Authors:\n",
    "\n",
    "* Sofiane LOUNICI [GitHub](https://github.com/sofianelounici)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data protection has become an important issue over the last few years. In such an environment, one of the most critical threats is represented by hardcoded (or plaintext) credentials in open-source projects. Several tools are already available to detect leaks in open-source platforms but the diversity of credentials (depending on multiple factors such as the programming language, code development conventions, or developers' personal habits) is a bottleneck for the effectiveness of these tools. Their lack of precision leads to a very high number of pieces of code detected as leaked secrets, even though they consist in perfectly legitimate code. Data wrongly detected as a leak is called *falsep ositive data*, and compose the huge majority of the data detected by currently available tools.\n",
    "\n",
    "The goal of Credential Digger is to reduce the amount of false positive data on the output of the scanning phase, by leveraging machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credential Digger relies on several components:\n",
    "\n",
    "* The **Regex Scanner** (or Scanner) is the basic component because it provides the regular expression scan on the GitHub repositories. In the installation, you configured a set of rules (from the .yml file) and it is still possible to add/remove your own rules\n",
    "\n",
    "* The **Path Model**: A lot of the discoveries are in example files such as documentations, README, etc. since it is very common for developers to provide test codes for their projects. The Path Model analyzes the *path* of each discovery and classifies it as false positive when needed\n",
    "\n",
    "* The **Snippet Models**: Now, we are tackling the most difficult part to detect a false positive: the code snippet. Two steps are required: a pre-processing step (called **Extractor**) and classification step (called **Classifier**). The output of the Classifier will contain the discoveries with a reduced amount of false positive.\n",
    "\n",
    "Finally, a user can review the filtered discoveries by *flagging* a discovery as false positive (see in later sections). The machine learning models can be enabled or disabled, depending on your use case. For more advanced users, it is possible to integrate your own model directly to Credential Digger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to show how to perform a basic scan, how to manage the repositories and the discoveries. When you launch Credential Digger, you will get the global dashboard:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/interface.PNG\" width=\"1000\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have several options:\n",
    "\n",
    "* You can scan a new repository, by clicking on **Scan Repo**\n",
    "* You can manage the rules you want to integrate for the scanner by clicking on **Rules**\n",
    "\n",
    "Let's say we want to scan a new repository. We click on **Scan Repo**, we select the category of rules we want (either API Keys, password, etc.), we select the URL of the GitHub repository we want, and we check the Machine Learning models that we want to apply to automatically reduce false positives. Then, we click on **Start Scan** to start the scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/scan_new_repo.PNG\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the scan is completed, you can go back to the dashboard and click on the repository you just scanned to see the list of the discoveries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/discoveries.PNG\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this repository we just scanned, we have 141 discoveries (a discovery with the state *new* needs to be reviewed). The *password* tag indicates the category of rules concerned by the discoveries. The button **Hide FPS** filter the discoveries by only considering the state *new*.\n",
    "\n",
    "For each discovery, you can :\n",
    "\n",
    "* **Open Commit**: you go directly to the source code to see the code snippet in context\n",
    "* **False positive**: you flag the discovery as false positive (modifying the state in the database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This basic tutorial is enough for the majority of users, who wants to manage their repository with an easy user interface while managing the false positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting this section, please make sure you have followed the instructions for **Advanced Install** in the [README](https://github.com/SAP/credential-digger/blob/main/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from credentialdigger import SqliteClient\n",
    "c = SqliteClient(path='mydata.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.get_repos() # Get the list of the repository currently in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.add_rules_from_file('../resources/rules.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scanning a repository is very easy. You specify the url of the repository and the models you want to apply. In this example, we choose a random repository: https://github.com/Mebus/cupp, and we are going to perform some analysis on the discoveries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:credentialdigger.client:Detected 141 discoveries.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of leaks:  141\n",
      "Number of false positives: 0\n"
     ]
    }
   ],
   "source": [
    "url = 'https://github.com/Mebus/cupp'\n",
    "discoveries = c.scan(repo_url=url,\n",
    "                     models=[])\n",
    "data = pd.DataFrame(c.get_discoveries(url))\n",
    "print(\"Number of leaks: \", len(data[data.state=='new']))\n",
    "print(\"Number of false positives:\", len(data[data.state!='new']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not ran the scan with the models, so no false positives have been identified.\n",
    "Another interesting analysis is to see the which regular expression detected the leak :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sshpass|password|pwd|passwd|pass in 100.0 %\n"
     ]
    }
   ],
   "source": [
    "for item in data.rule_id.unique():\n",
    "    percentage_occurences = len(data[data.rule_id==item])*100/len(data)\n",
    "    regex = c.get_rules()[item-1]['regex']\n",
    "    print(regex + ' in '+ str(percentage_occurences)+ \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scan the same repositories with the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:credentialdigger.client:Detected 53 discoveries.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of leaks:  209\n",
      "Number of false positives: 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "url = 'https://github.com/Mebus/cupp'\n",
    "discoveries = c.scan(repo_url=url,\n",
    "                     models=['PathModel', 'SnippetModel'])\n",
    "data = pd.DataFrame(c.get_discoveries(url))\n",
    "print(\"Number of leaks: \", len(data[data.state=='new']))\n",
    "print(\"Number of false positives:\", len(data[data.state!='new']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md +# CUPP - Common User Passwords Profiler\n",
      "cupp3.py +    pass\n",
      "cupp3.py +       \\\\   \\033[1;31m,__,\\033[1;m             # Passwords\n",
      "cupp3.py +    passwords = []\n",
      "cupp3.py +        passwords.append(row[6])\n",
      "cupp3.py +    gpa = sorted(set(passwords))\n",
      "cupp3.py +        passwords_file.write(os.linesep.join(gpa))\n",
      "test_cupp.py +        pass\n",
      "test_cupp.py +        pass\n",
      "cupp.py~ +\tpasswords = []\n",
      "README.md +# cupp.py - Common User Passwords Profiler\n",
      "README.md +  and a password or passphrase. If both match values stored within a locally\n",
      "README.md +  stored table, the user is authenticated for a connection. Password strength is\n",
      "README.md +  a measure of the difficulty involved in guessing or breaking the password\n",
      "README.md +  A weak password might be very short or only use alphanumberic characters,\n",
      "README.md +  making decryption simple. A weak password can also be one that is easily\n",
      "README.md +  name of a pet or relative, or a common word such as God, love, money or password.\n",
      "README.md +        -i      Interactive questions for user password profiling\n",
      "README.md +        -a      Parse default usernames and passwords directly from Alecto DB.\n",
      "README.md +# CUPP - Common User Passwords Profiler\n",
      "cupp3.py +    pass\n",
      "cupp3.py +       \\\\   \\033[1;31m,__,\\033[1;m             # Passwords\n",
      "cupp3.py +    passwords = []\n",
      "cupp3.py +        passwords.append(row[6])\n",
      "cupp3.py +    gpa = sorted(set(passwords))\n",
      "cupp3.py +        passwords_file.write(os.linesep.join(gpa))\n",
      "test_cupp.py +        pass\n",
      "test_cupp.py +        pass\n",
      "cupp.py~ +\tpasswords = []\n",
      "README.md +# cupp.py - Common User Passwords Profiler\n",
      "README.md +  and a password or passphrase. If both match values stored within a locally\n",
      "README.md +  stored table, the user is authenticated for a connection. Password strength is\n",
      "README.md +  a measure of the difficulty involved in guessing or breaking the password\n",
      "README.md +  A weak password might be very short or only use alphanumberic characters,\n",
      "README.md +  making decryption simple. A weak password can also be one that is easily\n",
      "README.md +  name of a pet or relative, or a common word such as God, love, money or password.\n",
      "README.md +        -i      Interactive questions for user password profiling\n",
      "README.md +        -a      Parse default usernames and passwords directly from Alecto DB.\n"
     ]
    }
   ],
   "source": [
    "for idx, row in data[data.state!='new'].iterrows():\n",
    "    print(row.file_name, row.snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, a lot of false positives rise in the README files, but also in regular python files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate your own models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can integrate your own models to Credential Digger. Similarly to the Path Model and to the Snippet Models, the input data is a row containing the path, code snippet, rule_id, etc. You have two options to integrate your own models:\n",
    "\n",
    "- You want to improve the current Path Model and Snippet Models. In this case, just replace the binaries in `credentialdigger/models_data` (be careful of the input data of your model and the output data)\n",
    "- You want to create a new model, working on a different type of data (like a new component). In this case, alongside your binaries, you need to follow the following process:\n",
    "\n",
    "### Process\n",
    "\n",
    "- Dedicate a new folder to the model, located in `credentialdigger/models` (e.g., `credentialdigger/models/path_model` for the `PathModel`)\n",
    "  - The class files (i.e., the implementation) should appear in this folder\n",
    "- The class must extend `BaseModel`:\n",
    "  - It is initialized with the name of the model and name of the binary files used for the classification of the discovery (e.g., the `PathModel` class requires the binary file called `model_path.bin` in the binary folder model `path_model`)\n",
    "  - It must override the `analyze` method. This method receives a discovery (python dictionary) as input, and returns a boolean as output (i.e., `True` if this discovery is classified as false positive)\n",
    "- **Update the [`__init__.py`](https://github.com/SAP/credential-digger/blob/main/credentialdigger/models/__init__.py) file** (the same way done for the `PathModel`)\n",
    "\n",
    "Refer to [`credentialdigger/models/path_model/path_model.py`](https://github.com/SAP/credential-digger/blob/main/credentialdigger/models/path_model/path_model.py) for an example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
